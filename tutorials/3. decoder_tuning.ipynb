{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: 基于CPM-Bee进行解码层微调（decoder tuning）\n",
    "本教程基于情感分类数据集SST2对CPM-Bee进行解码层微调（decoder tuning）。解码层微调（decoder tuning）是在不训练模型的情况下，通过加入输出端的解码器网络，使用少样本训练解码器网络来提升模型的理解能力。我们将4shots的微调结果与原始模型zero shot进行对比。\n",
    "\n",
    "This tutorial is based on the sentiment classification data set SST2 for CPM-Bee decoder tuning. decoder tuning is to improve the understanding ability of the model by joining the decoder network at the output end and training the decoder network with few samples without training the model. We compared the fine-tuning results of 4shots to the original model zero shot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据格式处理 (Process dataset)\n",
    "训练之前，我们需要定义并处理我们的数据输入格式，我们使用的原始样例数据如下\n",
    "\n",
    "Before training, we need to prepare and process our training data. Below is a piece of example training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example data\n",
    "# 4 shots\n",
    "train_data = [{\"input\": \"without shakespeare 's eloquent language , the update is dreary and sluggish . \",\"question\":\"What is the sentiment of this sentence?\",  \"<ans>\":\"bad\"},\n",
    "    {\"input\": \"final verdict : you 've seen it all before . \", \"question\":\"What is the sentiment of this sentence?\", \"<ans>\":\"bad\"},\n",
    "    {\"input\": \"the drama discloses almost nothing . \", \"question\":\"What is the sentiment of this sentence?\", \"<ans>\":\"bad\"},\n",
    "    {\"input\": \"an inexperienced director , mehta has much to learn . \", \"question\":\"What is the sentiment of this sentence?\", \"<ans>\":\"bad\"},\n",
    "    {\"input\": \"you live the mood rather than savour the story . \", \"question\":\"What is the sentiment of this sentence?\", \"<ans>\":\"great\"},\n",
    "    {\"input\": \"an edgy thriller that delivers a surprising punch . \", \"question\":\"What is the sentiment of this sentence?\", \"<ans>\":\"great\"},\n",
    "    {\"input\": \"nicholson 's understated performance is wonderful . \", \"question\":\"What is the sentiment of this sentence?\", \"<ans>\":\"great\"},\n",
    "    {\"input\": \"this is a gorgeous film - vivid with color , music and life . \", \"question\":\"What is the sentiment of this sentence?\", \"<ans>\":\"great\"},\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本教程中，我们使用的情感分类的输入格式如下（也可以自行定义其他格式）：\n",
    "\n",
    "In this tutorial, we use the following input format for emotion classification (you can also define other formats) :\n",
    "```\n",
    "input: text\n",
    "question: \"What is the sentiment of this sentence?\"\n",
    "<ans>: bad/great \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加工作路径\n",
    "Add working path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载test数据集，并且处理成类似与train_data的格式\n",
    "Load the test dataset and process it in a format similar to train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(dataset):\n",
    "    test_data = []\n",
    "    label_map = {\"0\":\"bad\", \"1\":\"great\"}\n",
    "    path = './decoder_tuning_data/raw_data/' + dataset.lower().split('-')[0]+ '/dev.tsv'\n",
    "    test_df = pd.read_csv(path, sep='\\t')\n",
    "    for index, row in test_df.iterrows():\n",
    "        example = {}\n",
    "        # print(row['sentence'], row['label'])\n",
    "        example[\"input\"] = row['sentence']\n",
    "        example[\"question\"] = \"What is the sentiment of this sentence?\"\n",
    "        example[\"<ans>\"] = label_map[str(row['label'])]\n",
    "        test_data.append(example)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_test_dataset('sst2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照预处理格式将处理好的数据存储为二进制文件\n",
    "The processed data is stored as binary files in a preprocessed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from cpm_live.dataset import build_dataset, shuffle_dataset\n",
    "\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bin_data(data, dataset_path,dataset_name):\n",
    "    output_path = \"./decoder_tuning_data/bin_data/\"  \n",
    "\n",
    "    with build_dataset(\"tmp\", \"data\") as dataset:\n",
    "        for item in data:\n",
    "            dataset.write(item) # reformat_data(item)\n",
    "    shuffle_dataset(\n",
    "        \"tmp\",\n",
    "        os.path.join(output_path, dataset_path),\n",
    "        progress_bar=True,\n",
    "        output_name=dataset_name\n",
    "    )\n",
    "    shutil.rmtree(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle step 1/2: 100%|██████████| 8/8 [00:00<00:00, 18117.94it/s]\n",
      "Shuffle step 2/2: 100%|██████████| 1/1 [00:00<00:00, 3905.31it/s]\n",
      "Shuffle step 1/2: 100%|██████████| 872/872 [00:00<00:00, 146022.80it/s]\n",
      "Shuffle step 2/2: 100%|██████████| 1/1 [00:00<00:00, 74.69it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"./decoder_tuning_data/bin_data\"):\n",
    "    os.system(\"rm -rf ./decoder_tuning_data/bin_data/\")\n",
    "if os.path.exists(\"./tmp\"):\n",
    "    os.system(\"rm -rf ./tmp\")\n",
    "build_bin_data(train_data, \"train_data\", \"example-data\")\n",
    "build_bin_data(test_data, \"test_data\", \"example-data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 训练（Training）\n",
    "我们自定义一个DecT_CPM类，方便实现在CPMBee上的decoder tuning，通过DecT_CPM中的run，run_zs函数分别实现4-shot下的decoder tuning和0-shot下的原本模型能力的测试。需要将预训练好的模型存储在`./ckpt/`文件夹下。具体而言，`./ckpt/`文件夹需要预先保存`./ckpt/config.json`、`./ckpt/pytorch_model.bin`和`./ckpt/vocab.txt`.\n",
    "\n",
    "We customize a DecT_CPM class to facilitate the realization of decoder tuning on CPMBee. Through the run and run_zs functions in DecT_CPM, decoder tuning under 4-shot and the original model ability test under 0-shot can be realized respectively. The pre-trained model needs to be stored in the './ckpt/ 'folder. Specifically, the './ckpt/ 'folder needs to be pre-stored'./ckpt/config.json ', './ckpt/pytorch_model.bin 'and'./ckpt/vocab.txt '."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一些超参数以及verbalizer\n",
    "Initialize some hyperparameters and verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一些超参数以及verbalizer\n",
    "label_map = {\"0\":\"bad\", \"1\":\"great\"}\n",
    "label_list = [\"bad\", \"great\"]\n",
    "lr = 4e-3\n",
    "proto_dim = 128\n",
    "model_logits_weight = 1\n",
    "max_epochs = 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造decoder_tuning的trainer\n",
    "Construct the decoder_tuning trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "import warnings\n",
    "from typing import Optional\n",
    "from typing import Callable, Union, Dict, List\n",
    "try:\n",
    "    from typing import OrderedDict\n",
    "except ImportError:\n",
    "    from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from copy import deepcopy\n",
    "\n",
    "class DecTCPM(object):\n",
    "    r\"\"\"A runner for DecT\n",
    "    This class is specially implemented for classification.\n",
    "    Decoder Tuning: Efficient Language Understanding as Decoding : https://arxiv.org/pdf/2212.08408.pdf\n",
    "\n",
    "    Args:\n",
    "        model (:obj:`CPMBeeTorch`): One ``CPMBeeTorch`` object.\n",
    "        test_dataloader (:obj:`FinetuneDataset`): The dataloader to bachify and process the test data.\n",
    "        tokenizer (:obj:`CPMBeeTokenizer`): The tokenizer to process the word.\n",
    "        verbalizer (:obj:`Verbalizer`): The verbalizer to map the label to the word.\n",
    "        device (:obj:`torch.device`): The device to run the model.\n",
    "        calibrate_dataloader (:obj:`FinetuneDataset`, optional): The dataloader that has empty input, to modify the output logits. Defaults to None.\n",
    "        lr (:obj:`float`, optional): The learning rate. Defaults to 5e-3.\n",
    "        hidden_size (:obj:`int`, optional): The hidden size of the model. Defaults to 4096.\n",
    "        mid_dim (:obj:`int`, optional): The dimension of the proto vector. Defaults to 128.\n",
    "        epochs (:obj:`int`, optional): The number of epochs to train. Defaults to 5.\n",
    "        model_logits_weight (:obj:`float`, optional): The weight of the model logits. Defaults to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 test_dataloader,\n",
    "                 tokenizer,\n",
    "                 verbalizer,\n",
    "                 device: Optional[Union[str, torch.device]] = \"cuda:0\",\n",
    "                 calibrate_dataloader: Optional[List] = None,\n",
    "                 lr: Optional[float] = 5e-3,\n",
    "                 hidden_size: Optional[int] = 4096,\n",
    "                 mid_dim: Optional[int] = 128,\n",
    "                 epochs: Optional[int] = 5,\n",
    "                 model_logits_weight: Optional[float] = 1,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.calibrate_dataloader = calibrate_dataloader\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.device = device\n",
    "        ids = []\n",
    "        for idx in range(len(verbalizer.items())):\n",
    "            ids.append(tokenizer.encode(verbalizer[str(idx)])[0][0])\n",
    "        self.label_list = list(verbalizer.values())\n",
    "        self.label_word_token_ids = []\n",
    "        for label_word in self.label_list:\n",
    "            self.label_word_token_ids.append(tokenizer.encode(label_word)[0][0])\n",
    "        self.ids = ids #nn.Parameter(torch.tensor(ids), requires_grad=False)\n",
    "        self.num_classes = len(self.ids)\n",
    "        self.lr = lr\n",
    "        self.mid_dim = mid_dim\n",
    "        self.epochs = epochs\n",
    "        self.model_logits_weight = model_logits_weight\n",
    "        self.hidden_dims = hidden_size\n",
    "        self.reset_parameter()\n",
    "    \n",
    "    # reset the parameters, useful when you want to test different random seeds\n",
    "    def reset_parameter(self):\n",
    "        self.head = nn.Linear(self.hidden_dims, self.mid_dim, bias=False)\n",
    "        w = torch.empty((self.num_classes, self.mid_dim)).to(self.device)\n",
    "        nn.init.xavier_uniform_(w)\n",
    "        self.proto = nn.Parameter(w, requires_grad=False)\n",
    "        r = torch.ones(self.num_classes)\n",
    "        self.proto_r = nn.Parameter(r, requires_grad=True)\n",
    "        self.optimizer = torch.optim.Adam([p for n, p in self.head.named_parameters()] + [self.proto_r], lr=self.lr)\n",
    "\n",
    "\n",
    "    # get the logits and hidden states of the model, specifically for cpmbee model, you can modify it for other models\n",
    "    def get_logits_and_hidden(self,data):\n",
    "        input_ids = torch.from_numpy(data[\"inputs\"]).cuda().to(torch.int32)\n",
    "        input_ids_sub = torch.from_numpy(data[\"inputs_sub\"]).cuda().to(torch.int32)\n",
    "        input_length = torch.from_numpy(data[\"length\"]).cuda().to(torch.int32)\n",
    "        input_context = torch.from_numpy(data[\"context\"]).cuda().bool()\n",
    "        input_sample_ids = torch.from_numpy(data[\"sample_ids\"]).cuda().to(torch.int32)\n",
    "        input_num_segments = torch.from_numpy(data[\"num_segments\"]).cuda().to(torch.int32)\n",
    "        input_segment_ids = torch.from_numpy(data[\"segment_ids\"]).cuda().to(torch.int32)\n",
    "        input_segment_rel_offset = (\n",
    "            torch.from_numpy(data[\"segment_rel_offset\"]).cuda().to(torch.int32)\n",
    "        )\n",
    "        input_segment_rel = torch.from_numpy(data[\"segment_rel\"]).cuda().to(torch.int32)\n",
    "        input_span = torch.from_numpy(data[\"spans\"]).cuda().to(torch.int32)\n",
    "        targets = torch.from_numpy(data[\"target\"]).cuda().to(torch.int32)\n",
    "        ext_table_ids = torch.from_numpy(data[\"ext_ids\"]).cuda().to(torch.int32)\n",
    "        ext_table_sub = torch.from_numpy(data[\"ext_sub\"]).cuda().to(torch.int32)\n",
    "        task_ids = torch.from_numpy(data[\"task_ids\"]).cuda().to(torch.int32)\n",
    "        task_names = data[\"task_names\"]\n",
    "        # to get the label from the targets\n",
    "        mask = torch.logical_or(targets ==self.ids[0], targets==self.ids[1])\n",
    "        labels = targets[mask]\n",
    "        final_label = []\n",
    "        for i in range(len(labels)):\n",
    "            final_label.append(self.ids.index(labels[i]))\n",
    "        with torch.no_grad():\n",
    "            logits, hidden_states = self.model(\n",
    "                    input_ids,\n",
    "                    input_ids_sub,\n",
    "                    input_length,\n",
    "                    input_context,\n",
    "                    input_sample_ids,\n",
    "                    input_num_segments,\n",
    "                    input_segment_ids,\n",
    "                    input_segment_rel_offset,\n",
    "                    input_segment_rel,\n",
    "                    input_span,\n",
    "                    ext_table_ids,\n",
    "                    ext_table_sub,\n",
    "                )\n",
    "        # mask the targets where value is -100 or 7, to get the index of the valid position\n",
    "        mask_matrix = deepcopy(targets)\n",
    "        mask_matrix[targets == -100] = 0\n",
    "        mask_matrix[targets == 7] = 0\n",
    "        index_mask = mask_matrix.nonzero(as_tuple=False)\n",
    "        # finally we get the logits and hidden states of the <ans> word position\n",
    "        filtered_logits = logits[index_mask[:, 0], index_mask[:, 1], :]\n",
    "        filtered_hiddens = hidden_states[index_mask[:, 0], index_mask[:, 1], :]\n",
    "        label_logits = filtered_logits[:,self.label_word_token_ids] # F.softmax(filtered_logits)[:,self.label_word_token_ids]\n",
    "        # normalize the hidden states to prevent the nan output in the loss\n",
    "        normalize_hidden = F.normalize(filtered_hiddens, p=2, dim=-1)\n",
    "        return label_logits, normalize_hidden,final_label\n",
    "    \n",
    "    # test the model on the dev set, if zs is true, then test on the zero-shot setting, otherwise test on the decoder tuning setting\n",
    "    def test(self, dataloader,zs):\n",
    "        if zs:\n",
    "            preds = []\n",
    "            labels = []\n",
    "            for iteration, data in enumerate(dataloader):\n",
    "                if data is None:\n",
    "                    if last_data is None:\n",
    "                        raise RuntimeError(\n",
    "                            \"Dataset is too small, please use a smaller batch size or sequence length!\"\n",
    "                        )\n",
    "                    data = last_data  # use last data\n",
    "                    skip_this_batch = True\n",
    "                else:\n",
    "                    last_data = data\n",
    "                logits,_,label = self.get_logits_and_hidden(data)\n",
    "                preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "                labels.extend(label)\n",
    "            res = sum([int(i==j) for i,j in zip(preds, labels)])/len(preds)\n",
    "            return res\n",
    "        else:\n",
    "            preds = []\n",
    "            labels = []\n",
    "            for iteration, data in enumerate(dataloader):\n",
    "                if data is None:\n",
    "                    if last_data is None:\n",
    "                        raise RuntimeError(\n",
    "                            \"Dataset is too small, please use a smaller batch size or sequence length!\"\n",
    "                        )\n",
    "                    data = last_data  # use last data\n",
    "                    skip_this_batch = True\n",
    "                else:\n",
    "                    last_data = data\n",
    "                logits,hidden_states,label = self.get_logits_and_hidden(data)\n",
    "                proto_logits = self.sim(self.head(F.normalize(hidden_states.float(),dim=-1)), self.proto, self.proto_r, logits.float(), self.model_logits_weight).cpu()\n",
    "                preds.extend(torch.argmax(proto_logits, dim=-1).cpu().tolist())\n",
    "                labels.extend(label)\n",
    "            res = sum([int(i==j) for i,j in zip(preds, labels)])/len(preds)\n",
    "            return res\n",
    "\n",
    "    @staticmethod\n",
    "    def sim(x, y, r=0, model_logits=0, model_logits_weight=1):\n",
    "        x = torch.unsqueeze(x, -2)\n",
    "        d = torch.norm((x - y), dim=-1)\n",
    "        dist = d - model_logits * model_logits_weight - r\n",
    "        return -dist\n",
    "    \n",
    "    # conduct the loss function in the decoder tuning\n",
    "    def loss_func(self, x, model_logits, labels):\n",
    "        sim_mat = torch.exp(self.sim(x, self.proto, self.proto_r, model_logits, self.model_logits_weight))\n",
    "        pos_score = torch.sum(sim_mat * F.one_hot(labels), -1)\n",
    "        loss = -torch.mean(torch.log(pos_score / sim_mat.sum(-1)))\n",
    "        return loss\n",
    "    \n",
    "    # run zero shot setting\n",
    "    def run_zs(self):\n",
    "        res = self.test(self.test_dataloader, zs = True)\n",
    "        print(\"zero shot acc:\",res)\n",
    "\n",
    "    # train the model with decoder tuning, you need to provide the training dataloader (type:FinetuneDataset)\n",
    "    def run(self, train_dataloader):\n",
    "        logits_list = []\n",
    "        hidden_states_list = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for iteration, data in enumerate(train_dataloader):\n",
    "                if data is None:\n",
    "                    if last_data is None:\n",
    "                        raise RuntimeError(\n",
    "                            \"Dataset is too small, please use a smaller batch size or sequence length!\"\n",
    "                        )\n",
    "                    data = last_data  # use last data\n",
    "                    skip_this_batch = True\n",
    "                else:\n",
    "                    last_data = data\n",
    "                train_logits, train_embeds,label = self.get_logits_and_hidden(data)\n",
    "                logits_list.append(train_logits)\n",
    "                hidden_states_list.append(train_embeds)\n",
    "                labels.extend(label)\n",
    "        train_logits = torch.cat(logits_list,dim=0)\n",
    "        train_embeds = torch.cat(hidden_states_list,dim=0)\n",
    "        embeds = [[] for _ in range(self.num_classes)]\n",
    "        train_labels = [[] for _ in range(self.num_classes)]\n",
    "        model_logits = [[] for _ in range(self.num_classes)]\n",
    "        total_num = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for idx, label in enumerate(labels):\n",
    "            label = torch.tensor(label)\n",
    "            train_labels[label].append(label)\n",
    "            embeds[label].append(torch.tensor(train_embeds[idx]))\n",
    "            model_logits[label].append(torch.tensor(train_logits[idx]))\n",
    "        embeds = list(map(torch.stack, embeds))\n",
    "        labels = torch.cat(list(map(torch.stack, train_labels))).to(self.device)\n",
    "        model_logits = torch.cat(list(map(torch.stack, model_logits))).float()\n",
    "\n",
    "        self.head.to(self.device)\n",
    "        self.proto.to(self.device)\n",
    "        self.proto_r.to(self.device)\n",
    "        dist = list(map(lambda x: torch.norm(self.head(x.float()) - self.head(x.float().mean(0)), dim=-1).mean(), embeds))\n",
    "        self.proto_r.data = torch.stack(dist)\n",
    "        \n",
    "        loss = 0.\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            x = self.head(torch.cat(embeds).float())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss_func(x, model_logits, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(\"Total epoch: {}. DecT loss: {}\".format(epoch, loss))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"Training time: {}\".format(end_time - start_time))\n",
    "        res= self.test(self.test_dataloader, zs = False)\n",
    "        print(\"dect acc:\",res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型的权重并部署\n",
    "Load the model's weights and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CPMBeeTorch(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (24): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (25): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (26): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (27): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (28): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (29): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (30): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (31): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (32): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (33): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (34): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (35): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (36): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (37): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (38): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (39): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (40): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (41): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (42): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (43): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (44): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (45): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (46): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (47): TransformerBlock(\n",
       "        (self_att): SelfAttentionBlock(\n",
       "          (layernorm_before_attention): LayerNorm()\n",
       "          (self_attention): Attention(\n",
       "            (project_q): Linear()\n",
       "            (project_k): Linear()\n",
       "            (project_v): Linear()\n",
       "            (attention_out): Linear()\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ffn): FFNBlock(\n",
       "          (layernorm_before_ffn): LayerNorm()\n",
       "          (ffn): FeedForward(\n",
       "            (w_in): DenseGatedACT(\n",
       "              (w_0): Linear()\n",
       "              (w_1): Linear()\n",
       "              (act): GELU()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (w_out): Linear()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_layernorm): LayerNorm()\n",
       "  )\n",
       "  (input_embedding): EmbeddingExt(\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "  )\n",
       "  (position_bias): BucketPositionBias()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpm_live.tokenizers import CPMBeeTokenizer\n",
    "from cpm_live.training_tasks.bee import FinetuneDataset\n",
    "from cpm_live.models import CPMBeeConfig, CPMBeeTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import bmtrain as bmt\n",
    "from copy import deepcopy\n",
    "config = CPMBeeConfig.from_json_file(\"./ckpt/config.json\")\n",
    "ckpt_path = \"./ckpt/pytorch_model.bin\"\n",
    "tokenizer = CPMBeeTokenizer()\n",
    "model = CPMBeeTorch(config=config)\n",
    "model.load_state_dict(torch.load(ckpt_path), strict=False)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建dataloader\n",
    "build dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = FinetuneDataset(\n",
    "        dataset_path = \"./decoder_tuning_data//bin_data/train_data\",\n",
    "        batch_size=4,\n",
    "        max_length=512,\n",
    "        max_depth=8,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "test_dataloader = FinetuneDataset(\n",
    "        dataset_path = \"./decoder_tuning_data/bin_data/test_data\",\n",
    "        batch_size=4,\n",
    "        max_length=512,\n",
    "        max_depth=8,\n",
    "        tokenizer=tokenizer,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建runner\n",
    "Build runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = DecTCPM(\n",
    "    model = model,\n",
    "    test_dataloader = test_dataloader,\n",
    "    tokenizer = tokenizer,\n",
    "    verbalizer = label_map,\n",
    "    device = device,\n",
    "    calibrate_dataloader = None,\n",
    "    lr = lr,\n",
    "    mid_dim = proto_dim,\n",
    "    epochs = max_epochs,\n",
    "    model_logits_weight = model_logits_weight,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练，最后输出的是zero shot和decoder tuning的准确率\n",
    "Start training, the final output is zero shot and decoder tuning accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero shot acc: 0.8623853211009175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_484737/3776135156.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeds[label].append(torch.tensor(train_embeds[idx]))\n",
      "/tmp/ipykernel_484737/3776135156.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_logits[label].append(torch.tensor(train_logits[idx]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch: 0. DecT loss: 0.4929628372192383\n",
      "Total epoch: 1. DecT loss: 0.34562215209007263\n",
      "Total epoch: 2. DecT loss: 0.3390842080116272\n",
      "Total epoch: 3. DecT loss: 0.3364109992980957\n",
      "Total epoch: 4. DecT loss: 0.33365967869758606\n",
      "Total epoch: 5. DecT loss: 0.3307572603225708\n",
      "Total epoch: 6. DecT loss: 0.3278043270111084\n",
      "Total epoch: 7. DecT loss: 0.32488542795181274\n",
      "Total epoch: 8. DecT loss: 0.32205426692962646\n",
      "Total epoch: 9. DecT loss: 0.3193338215351105\n",
      "Total epoch: 10. DecT loss: 0.31672176718711853\n",
      "Total epoch: 11. DecT loss: 0.31419551372528076\n",
      "Total epoch: 12. DecT loss: 0.3117189109325409\n",
      "Total epoch: 13. DecT loss: 0.3092496395111084\n",
      "Total epoch: 14. DecT loss: 0.30674344301223755\n",
      "Total epoch: 15. DecT loss: 0.3041604459285736\n",
      "Total epoch: 16. DecT loss: 0.3014650344848633\n",
      "Total epoch: 17. DecT loss: 0.29862841963768005\n",
      "Total epoch: 18. DecT loss: 0.2956272065639496\n",
      "Total epoch: 19. DecT loss: 0.2924429774284363\n",
      "Total epoch: 20. DecT loss: 0.28906041383743286\n",
      "Total epoch: 21. DecT loss: 0.2854648232460022\n",
      "Total epoch: 22. DecT loss: 0.2816403806209564\n",
      "Total epoch: 23. DecT loss: 0.2775668799877167\n",
      "Total epoch: 24. DecT loss: 0.2732172906398773\n",
      "Total epoch: 25. DecT loss: 0.2685588002204895\n",
      "Total epoch: 26. DecT loss: 0.2635546922683716\n",
      "Total epoch: 27. DecT loss: 0.2581753730773926\n",
      "Total epoch: 28. DecT loss: 0.2524105906486511\n",
      "Total epoch: 29. DecT loss: 0.24628719687461853\n",
      "Training time: 0.17364883422851562\n",
      "dect acc: 0.8990825688073395\n"
     ]
    }
   ],
   "source": [
    "runner.run_zs()\n",
    "runner.run(train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
